{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lbrTdjG6LUx",
        "outputId": "f9a1c85b-dd3e-4481-f372-6494afd55012"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/ColumbiaMSCS/COMS4995_DL_for_CV/Project/Data/project_data.zip\" \"/content/project_data.zip\""
      ],
      "metadata": {
        "id": "vTZ48PYK4CN-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/data\""
      ],
      "metadata": {
        "id": "PL4QA5Uo4O-w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/project_data.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "id": "8lThtVk74SGa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import models\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "QCT0LhmvVcLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of each class in the data\n",
        "project_data = {}\n",
        "\n",
        "project_data['spring'] = glob.glob('/content/data/project_data/spring/spring*')\n",
        "project_data['summer'] = glob.glob('/content/data/project_data/summer/summer*')\n",
        "project_data['fall'] = glob.glob('/content/data/project_data/fall/fall*')\n",
        "project_data['winter'] = glob.glob('/content/data/project_data/winter/winter*')\n",
        "\n",
        "print(f\"count of spring images :  {len(project_data['spring'])}\")\n",
        "print(f\"count of summer images :  {len(project_data['summer'])}\")\n",
        "print(f\"count of fall images :  {len(project_data['fall'])}\")\n",
        "print(f\"count of winter images :  {len(project_data['winter'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Rea0SHbUl9",
        "outputId": "49d35d87-0aa4-43d5-bfa5-036dd2b15d69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of spring images :  6000\n",
            "count of summer images :  6000\n",
            "count of fall images :  6000\n",
            "count of winter images :  6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = '/content/data/project_data'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-qC3fENvcDkd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "dataset = datasets.ImageFolder(root=SOURCE, transform=transform)"
      ],
      "metadata": {
        "id": "-CHlDm5lcWGv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 4,200 images of each category for traning, 900 of each category for validation, 900 images of each category for testing\n",
        "fall_samples = []\n",
        "spring_samples = []\n",
        "summer_samples = []\n",
        "winter_samples = []\n",
        "\n",
        "# Separate dataset into four label groups\n",
        "for i in range(len(dataset)):\n",
        "    _, label = dataset[i]\n",
        "    if label == 0:\n",
        "        fall_samples.append(i)\n",
        "    elif label == 1:\n",
        "        spring_samples.append(i)\n",
        "    elif label == 2:\n",
        "        summer_samples.append(i)\n",
        "    else:\n",
        "        winter_samples.append(i)\n",
        "\n",
        "# Shuffle the samples\n",
        "random.shuffle(fall_samples)\n",
        "random.shuffle(spring_samples)\n",
        "random.shuffle(summer_samples)\n",
        "random.shuffle(winter_samples)\n",
        "\n",
        "# Split the samples\n",
        "train_indices = fall_samples[:4200] + spring_samples[:4200] + summer_samples[:4200] + winter_samples[:4200]\n",
        "val_indices = fall_samples[4200:5100] + spring_samples[4200:5100] + summer_samples[4200:5100] + winter_samples[4200:5100]\n",
        "test_indices = fall_samples[5100:6000] + spring_samples[5100:6000] + summer_samples[5100:6000] + winter_samples[5100:6000]\n",
        "\n",
        "# Shuffle the indices\n",
        "random.shuffle(train_indices)\n",
        "random.shuffle(val_indices)\n",
        "random.shuffle(test_indices)\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)"
      ],
      "metadata": {
        "id": "nemoJyjggLiq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "FY4njLdDimG0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained GoogLeNet\n",
        "model_googlenet = googlenet(weights=GoogLeNet_Weights.DEFAULT)"
      ],
      "metadata": {
        "id": "wWCRScjHisf0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the final fully connected layer's output feature size to 4 so that\n",
        "# it's suitable for softmax activation for classification on four seasons\n",
        "model_googlenet.fc.out_features = 4"
      ],
      "metadata": {
        "id": "oTEOPAeZjuB8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the last modified layer\n",
        "for name, param in model_googlenet.named_parameters():\n",
        "    if \"fc\" not in name:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True"
      ],
      "metadata": {
        "id": "aUZCIKNGkIH4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU\n",
        "model_googlenet = model_googlenet.to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_googlenet.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "softmax = nn.Softmax(dim=1)"
      ],
      "metadata": {
        "id": "zRDN0XOekN21"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs, threshold):\n",
        "    best_val_loss = float('inf')\n",
        "    degrade_times = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        train_count = 0\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * len(y_batch)\n",
        "            pred = softmax(outputs)\n",
        "            train_corrects += (torch.argmax(pred, dim=1) == y_batch).float().sum()\n",
        "            train_count += y_batch.size(0)\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_corrects / train_count\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_count = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "                outputs = model(x_batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "                val_loss += loss.item() * len(y_batch)\n",
        "                pred = softmax(outputs)\n",
        "                val_corrects += (torch.argmax(pred, dim=1) == y_batch).float().sum()\n",
        "                val_count += y_batch.size(0)\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_corrects / val_count\n",
        "        print(f'Epoch {epoch} Train Loss {train_loss:.4f} Train Accuracy {train_acc:.4f} Validation Loss {val_loss:.4f} Validation Accuracy {val_acc:.4f}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            degrade_times = 0\n",
        "            # Save the model if it has the best validation loss so far\n",
        "            torch.save(model.state_dict(), './best_model_googlenet.pth')\n",
        "        else:\n",
        "            degrade_times += 1\n",
        "            # If the number of epochs where validation loss continuously increases\n",
        "            # is larger than threshold, stop training the network to avoid overfitting\n",
        "            if degrade_times > threshold:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break"
      ],
      "metadata": {
        "id": "MI9cMwvWkl6G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "            outputs = model(x_batch)\n",
        "            pred = softmax(outputs)\n",
        "            correct += (torch.argmax(pred, dim=1) == y_batch).float().sum()\n",
        "            count += y_batch.size(0)\n",
        "        test_acc = correct / count\n",
        "    print(f'Accuracy on test set: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "TpMswxJ0lY7u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the network on seasons image data\n",
        "train_model(model_googlenet, train_loader, val_loader, loss_fn, optimizer, epochs=10, threshold=2)"
      ],
      "metadata": {
        "id": "gWBQIH14llxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bebd6ec-1091-4d04-b027-96ab3a40b6ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train Loss 1.4309 Train Accuracy 0.5181 Validation Loss 0.9463 Validation Accuracy 0.6403\n",
            "Epoch 1 Train Loss 0.9442 Train Accuracy 0.6284 Validation Loss 0.8505 Validation Accuracy 0.6706\n",
            "Epoch 2 Train Loss 0.8828 Train Accuracy 0.6510 Validation Loss 0.8076 Validation Accuracy 0.6917\n",
            "Epoch 3 Train Loss 0.8430 Train Accuracy 0.6660 Validation Loss 0.7843 Validation Accuracy 0.6981\n",
            "Epoch 4 Train Loss 0.8306 Train Accuracy 0.6704 Validation Loss 0.7755 Validation Accuracy 0.6997\n",
            "Epoch 5 Train Loss 0.8235 Train Accuracy 0.6757 Validation Loss 0.7623 Validation Accuracy 0.7058\n",
            "Epoch 6 Train Loss 0.8167 Train Accuracy 0.6756 Validation Loss 0.7536 Validation Accuracy 0.7106\n",
            "Epoch 7 Train Loss 0.8035 Train Accuracy 0.6826 Validation Loss 0.7481 Validation Accuracy 0.7103\n",
            "Epoch 8 Train Loss 0.8002 Train Accuracy 0.6784 Validation Loss 0.7538 Validation Accuracy 0.7086\n",
            "Epoch 9 Train Loss 0.7973 Train Accuracy 0.6848 Validation Loss 0.7529 Validation Accuracy 0.7089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model_googlenet.load_state_dict(torch.load('./best_model_googlenet.pth'))\n",
        "# Evaluate on the test set\n",
        "evaluate_model(model_googlenet, test_loader)"
      ],
      "metadata": {
        "id": "lWFoX0OrlrVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08681d8-9888-4ce4-ca08-beff7059dd78"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.6958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze all layers\n",
        "for param in model_googlenet.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "I27Xqc4Ply3Q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the optimizer with a smaller learning rate\n",
        "optimizer = optim.SGD(model_googlenet.parameters(), lr=0.0005, momentum=0.9)\n",
        "# Continue fine-tuning the network\n",
        "train_model(model_googlenet, train_loader, val_loader, loss_fn, optimizer, epochs=10, threshold=2)"
      ],
      "metadata": {
        "id": "uZ_sW3XEl0sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bee799-d3f2-4301-f977-77ab74c54538"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train Loss 0.7518 Train Accuracy 0.7059 Validation Loss 0.6652 Validation Accuracy 0.7397\n",
            "Epoch 1 Train Loss 0.6448 Train Accuracy 0.7493 Validation Loss 0.6402 Validation Accuracy 0.7508\n",
            "Epoch 2 Train Loss 0.5785 Train Accuracy 0.7758 Validation Loss 0.6216 Validation Accuracy 0.7597\n",
            "Epoch 3 Train Loss 0.5286 Train Accuracy 0.7980 Validation Loss 0.6205 Validation Accuracy 0.7586\n",
            "Epoch 4 Train Loss 0.4741 Train Accuracy 0.8174 Validation Loss 0.6189 Validation Accuracy 0.7614\n",
            "Epoch 5 Train Loss 0.4225 Train Accuracy 0.8410 Validation Loss 0.6297 Validation Accuracy 0.7628\n",
            "Epoch 6 Train Loss 0.3822 Train Accuracy 0.8596 Validation Loss 0.6445 Validation Accuracy 0.7597\n",
            "Epoch 7 Train Loss 0.3353 Train Accuracy 0.8780 Validation Loss 0.6669 Validation Accuracy 0.7575\n",
            "Early stopping at epoch 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model_googlenet.load_state_dict(torch.load('./best_model_googlenet.pth'))\n",
        "# Evaluate on the test set\n",
        "evaluate_model(model_googlenet, test_loader)"
      ],
      "metadata": {
        "id": "KBZljY9ol_u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a01fb9-10bd-46c6-9701-952ecebf05c1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.7372\n"
          ]
        }
      ]
    }
  ]
}